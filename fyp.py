# -*- coding: utf-8 -*-
"""FYP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WRwp1FSl1ZhqyDZ5ZyXjTEGkO3i7BVy_

## **Music Features Detection (F,A,Phase)**
"""

from google.colab import files
import librosa
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Audio
import librosa.display

# Step 1: Upload an audio file
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]  # Get the uploaded file name

# Step 2: Play the audio file
Audio(audio_path)

# Step 3: Load the audio file
y, sr = librosa.load(audio_path)

# Step 4: Compute the Short-Time Fourier Transform (STFT) of the audio
stft = librosa.stft(y)
amplitude = np.abs(stft)  # Amplitude
phase = np.angle(stft)    # Phase
frequencies = np.linspace(0, sr / 2, amplitude.shape[0])

# Step 5: Plot Amplitude, Frequency, and Phase
plt.figure(figsize=(10, 6))

# Amplitude plot
plt.subplot(3, 1, 1)
plt.title('Amplitude (Magnitude Spectrogram)')
librosa.display.specshow(librosa.amplitude_to_db(amplitude, ref=np.max), sr=sr, y_axis='log', x_axis='time')
plt.colorbar(format='%+2.0f dB')

# Frequency plot
plt.subplot(3, 1, 2)
plt.title('Frequency over Time')
plt.plot(np.linspace(0, len(y)/sr, len(amplitude[0])), np.mean(amplitude, axis=0))
plt.xlabel('Time (s)')
plt.ylabel('Frequency (Hz)')

"""## **2nd Part (Clip to PDF)**

### **Audio Preprocessing Script**
"""

# Step 1: Install dependencies (latest version of FFmpeg)
print("Installing FFmpeg (latest version)...")
!apt-get update -qq
!apt-get install -y ffmpeg

# Verify FFmpeg version
print("\nVerifying installed FFmpeg version...")
!ffmpeg -version | head -n 1
print("FFmpeg installed and verified.\n")

# Step 2: Preprocess Audio with FFmpeg
def preprocess_audio(input_file):
    print("Step 2: Preprocess Audio with FFmpeg")
    output_file = "preprocessed_audio.wav"

    # Convert to mono WAV, 44.1 kHz, 16-bit
    !ffmpeg -i "$input_file" -ac 1 -ar 44100 -acodec pcm_s16le "$output_file" -y

    if not os.path.exists(output_file):
        raise RuntimeError("FFmpeg preprocessing failed. Output file not created.")

    print("Preprocessing complete. Downloading preprocessed audio...")
    files.download(output_file)
    return output_file

# Execute Step 2 using previously uploaded file
try:
    preprocessed_file = preprocess_audio(audio_path)
    print("\nStep 2 completed successfully!")
except Exception as e:
    print(f"An error occurred: {str(e)}")

"""### **Audio Separation with Demucs**"""

import shutil

print("Installing Demucs (latest version)...")
!pip install --upgrade pip -q
!pip install demucs -q --upgrade   # model

# Verify Demucs version
print("\nVerifying installed Demucs version...")
!pip show demucs | grep Version
print("Demucs installed and verified.\n")

# Step 2: Separate Other with Demucs
def separate_other():
    print("\nStep 2: Separate Other with Demucs")

    # Define the path to the preprocessed audio in Colab storage
    preprocessed_audio_path = "/content/preprocessed_audio.wav"

    if not os.path.exists(preprocessed_audio_path):
        raise FileNotFoundError(f"Preprocessed audio file not found at {preprocessed_audio_path}. Ensure it exists before running this step.")

    output_dir = "demucs_output"

    # Run Demucs to separate the "other" track
    print(f"Processing {preprocessed_audio_path} with Demucs...")
    !demucs --two-stems other "$preprocessed_audio_path" -o "$output_dir"

    # The output "other" track is usually in a subdirectory
    other_file = os.path.join(output_dir, "htdemucs", "preprocessed_audio", "other.wav")
    if not os.path.exists(other_file):
        raise RuntimeError(f"Demucs separation failed. 'Other' track not found at {other_file}. Check output directory: {os.listdir(output_dir)}")

    # Rename and move the file for easier access
    separated_other_file = "separated_other.wav"
    print(f"Copying {other_file} to {separated_other_file}...")
    shutil.copy(other_file, separated_other_file)

    # Verify the copied file exists
    if not os.path.exists(separated_other_file):
        raise RuntimeError(f"Failed to copy file to {separated_other_file}. Check permissions or disk space.")

    print(f"File size of {separated_other_file}: {os.path.getsize(separated_other_file)} bytes")
    print("Other separation complete. Attempting to download separated 'other' audio...")

    try:
        from google.colab import files
        files.download(separated_other_file)
        print(f"Successfully downloaded {separated_other_file}")
    except Exception as download_error:
        print(f"Download failed: {str(download_error)}. File is available at {separated_other_file}. You can manually download it or check Colab runtime.")

    return separated_other_file

# Execute Step 2
try:
    separate_other()
    print("\nStep 2 completed successfully!")
except Exception as e:
    print(f"An error occurred: {str(e)}")

"""### **Extract only Guitar From Others**"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow librosa matplotlib numpy soundfile

import os
import numpy as np
import librosa
import librosa.display
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import soundfile as sf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# ------------------- Feature Extraction & Data Augmentation -------------------

def extract_spectrogram(file_path):
    """Convert audio file to Mel Spectrogram and return as an image-like array."""
    y, sr = librosa.load(file_path, sr=22050)  # Load audio
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    # Ensure the spectrogram has the correct size (128x128)
    mel_spec_resized = np.resize(mel_spec_db, (128, 128))

    return mel_spec_resized, y, sr

def augment_audio(y, sr):
    """Applies random pitch shift, time stretch, and noise to audio."""
    if np.random.rand() < 0.5:
        y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=np.random.uniform(-1.5, 1.5))  # Less extreme pitch shift
    if np.random.rand() < 0.5:
        y = librosa.effects.time_stretch(y, rate=np.random.uniform(0.9, 1.1))  # Less extreme time stretch
    if np.random.rand() < 0.5:
        noise = np.random.randn(len(y)) * 0.002  # Lower noise level
        y = y + noise
    return y

# ------------------- Load Dataset with Augmentation -------------------

def load_data(guitar_dir, non_guitar_dir):
    """Load dataset, apply augmentation, extract spectrograms, and assign labels."""
    features, labels = [], []

    for folder, label in [(guitar_dir, 1), (non_guitar_dir, 0)]:
        for file in os.listdir(folder):
            if file.endswith(('.wav', '.mp3')):
                file_path = os.path.join(folder, file)

                # Original Data
                spectrogram, y, sr = extract_spectrogram(file_path)
                features.append(spectrogram)
                labels.append(label)

                # Augmented Data (Apply augmentation only to training data)
                if np.random.rand() < 0.7:  # Apply augmentation to 70% of files
                    y_aug = augment_audio(y, sr)
                    spectrogram_aug, _, _ = extract_spectrogram(file_path)
                    features.append(spectrogram_aug)
                    labels.append(label)

    X, y = np.array(features), np.array(labels)
    X = X.reshape(X.shape[0], 128, 128, 1)  # Reshape for CNN (128x128x1)
    y = to_categorical(y, 2)  # Convert labels to one-hot encoding

    return X, y

# Paths to dataset
guitar_dir = '/content/drive/MyDrive/extra/guitar_dataset'
non_guitar_dir = '/content/drive/MyDrive/extra/non_guitar_dataset'

# Load data
X, y = load_data(guitar_dir, non_guitar_dir)

# Ensure dataset is not empty before splitting
if len(X) == 0:
    raise ValueError("Dataset is empty. Check your file paths and data loading process.")

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# ------------------- Optimized CNN Model -------------------

def build_optimized_cnn():
    """Create an optimized CNN model with better regularization."""
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),

        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.5),  # Extra dropout added

        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.5),  # Extra dropout added

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(2, activation='softmax')  # 2 output classes (guitar / non-guitar)
    ])

    model.compile(optimizer=Adam(learning_rate=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Train model
model = build_optimized_cnn()

# Callbacks to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)

# Train model with callbacks
history = model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test),
                    callbacks=[early_stopping, reduce_lr])

# ------------------- Plot Training & Validation Loss -------------------

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training & Validation Loss")
plt.legend()
plt.show()

# ------------------- Model Evaluation -------------------

loss, acc = model.evaluate(X_test, y_test)
print(f"Model Accuracy: {acc:.4f}")

# ------------------- Confusion Matrix -------------------

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true_classes, y_pred_classes)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-Guitar", "Guitar"], yticklabels=["Non-Guitar", "Guitar"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("\nClassification Report:\n", classification_report(y_true_classes, y_pred_classes, target_names=["Non-Guitar", "Guitar"]))

# ------------------- Detect Guitar Sound & Extract It -------------------

def detect_and_save_guitar(input_file, output_file="guitar.wav", segment_length=1.0, threshold=0.6):
    """Detect guitar in an audio file and save the guitar segments if found."""
    print(f"Processing file: {input_file}")

    spectrogram, y, sr = extract_spectrogram(input_file)
    spectrogram = spectrogram.reshape(1, 128, 128, 1)

    prediction_prob = model.predict(spectrogram)[0][1]

    if prediction_prob >= threshold:
        print(f"Guitar Detected (Confidence: {prediction_prob:.2f})")
        sf.write(output_file, y, sr)
    else:
        print(f"No Guitar Sound Detected (Confidence: {prediction_prob:.2f})")

# Run Guitar Detection
new_audio_file = "separated_other.wav"
detect_and_save_guitar(new_audio_file)

# from google.colab import files
# files.download('guitar.wav')

"""### **Music Transcription with Librosa & Pretty MIDI**"""

!pip install pretty_midi

import pretty_midi

def transcribe_notes():
    print("\nStep 3: Transcribe Notes with Librosa and Pretty MIDI")

    # Check for available file
    if os.path.exists("guitar.wav"):
        input_file = "guitar.wav"
    elif os.path.exists("separated_other.wav"):
        input_file = "separated_other.wav"
    else:
        raise FileNotFoundError("Neither 'guitar.wav' nor 'separated_other.wav' was found in the Colab environment.")

    print(f"Using file: {input_file}")

    output_midi = "transcribed_notes.mid"

    print("Loading audio file...")
    audio, sr = librosa.load(input_file, sr=44100)

    # Detect pitch using PYIN
    print("Detecting pitch with PYIN...")
    f0, voiced_flag, _ = librosa.pyin(
        audio,
        fmin=librosa.note_to_hz('C2'),
        fmax=librosa.note_to_hz('C7'),
        sr=sr
    )

    # Detect onsets
    print("Detecting onsets...")
    onset_times = librosa.onset.onset_detect(y=audio, sr=sr, units='time')

    # Ensure we have detected notes
    if len(onset_times) < 2:
        raise RuntimeError("Not enough onsets detected to generate MIDI.")

    # Create MIDI file
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=25)  # Nylon String Guitar

    print("Converting detected pitches and onsets to MIDI...")

    for i in range(len(onset_times) - 1):
        start_time = onset_times[i]
        end_time = onset_times[i + 1]

        # Get the median frequency in this time window
        start_idx = librosa.time_to_frames(start_time, sr=sr)
        end_idx = librosa.time_to_frames(end_time, sr=sr)

        # Extract valid (voiced) f0 values in this window
        valid_f0 = f0[start_idx:end_idx][voiced_flag[start_idx:end_idx]]

        if len(valid_f0) == 0:
            continue  # Skip unvoiced segments

        median_f0 = np.nanmedian(valid_f0)
        if np.isnan(median_f0):
            continue  # Skip if no valid frequency

        midi_note = int(round(librosa.hz_to_midi(median_f0)))
        if 0 <= midi_note <= 127:
            # Determine velocity based on amplitude
            velocity = min(127, max(40, int(np.mean(np.abs(audio[start_idx:end_idx])) * 5000)))

            note = pretty_midi.Note(
                velocity=velocity,
                pitch=midi_note,
                start=start_time,
                end=end_time
            )
            instrument.notes.append(note)

    if not instrument.notes:
        raise RuntimeError("No valid notes detected. MIDI file will be empty.")

    midi.instruments.append(instrument)

    print("Saving MIDI file...")
    midi.write(output_midi)

    if os.path.getsize(output_midi) == 0:
        raise RuntimeError(f"Failed to create MIDI file at {output_midi}.")

    print("Transcription complete. Downloading MIDI file...")
    try:
        files.download(output_midi)
        print(f"Successfully downloaded {output_midi}")
    except Exception as download_error:
        print(f"Download failed: {str(download_error)}. File is available at {output_midi}. You can manually download it.")

    return output_midi

# Execute Step 3
try:
    transcribe_notes()
    print("\nStep 3 completed successfully!")
except Exception as e:
    print(f"An error occurred: {str(e)}")

"""### **MIDI to PDF || MusicXML**"""

# Install required libraries
!pip install music21 pretty_midi

import music21
from IPython.display import display, HTML

def convert_midi_to_pdf(midi_file_path):
    """
    Convert a MIDI file to a fixed PDF music notation file named 'generated_notation.pdf'.
    """
    print(f"Processing {midi_file_path}...")

    # Parse the MIDI file
    try:
        # Ensure music21 clears cache to avoid duplication
        music21.environment.set('autoDownload', 'allow')

        # Load the MIDI file
        score = music21.converter.parse(midi_file_path)

        # Create output directory if it doesn't exist
        output_dir = "/content"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # Define a fixed filename
        pdf_path = f"{output_dir}/generated_notation.pdf"

        # Convert to PDF
        try:
            score.write('musicxml.pdf', fp=pdf_path)
            print(f" PDF score created successfully at '{pdf_path}'")

            # Provide download link for Colab
            files.download(pdf_path)
            return True

        except Exception as e:
            print("Direct PDF conversion failed. Trying alternative method...")

            # Save as MusicXML first
            musicxml_path = f"{output_dir}/generated_notation.musicxml"

            # Ensure the file is saved properly before downloading
            score.write('musicxml', fp=musicxml_path)

            if os.path.exists(musicxml_path):  # Check if the file is actually saved
                print(f"MusicXML file saved at '{musicxml_path}'")
            else:
                print("Failed to save MusicXML file in Colab.")

            print("Unable to create PDF directly. Providing MusicXML file instead.")
            print("You can convert this file to PDF using any MusicXML-compatible notation software.")

            # Provide download link for MusicXML
            files.download(musicxml_path)

            return False

    except Exception as e:
        print(f"Error processing MIDI file: {str(e)}")
        return False

# Use the predefined MIDI file from Google Colab
midi_file_path = "transcribed_notes.mid"

# Check if the file exists before processing
if os.path.exists(midi_file_path):
    convert_midi_to_pdf(midi_file_path)
else:
    print(f" MIDI file '{midi_file_path}' not found. Please ensure it exists in the working directory.")

"""### **MusicXML to PDF**"""

# Install required libraries
!pip install music21

# Install MuseScore
!apt-get update
!apt-get install -y musescore3

import music21
import subprocess

def convert_musicxml_to_pdf():
    """
    Check for MusicXML files in the current directory and convert the first one to PDF.
    """
    # Find .musicxml or .xml files in the current directory
    musicxml_files = [f for f in os.listdir('.') if f.endswith('.musicxml') or f.endswith('.xml')]

    if not musicxml_files:
        print("No Need for This! PDF Already Generated")
        return

    filename = musicxml_files[0]  # Process the first found file
    print(f"Processing {filename}...")

    # Create output directory
    output_dir = "pdf_output"
    os.makedirs(output_dir, exist_ok=True)

    # Base filename without extension
    base_filename = os.path.splitext(os.path.basename(filename))[0]
    pdf_path = f"{output_dir}/{base_filename}.pdf"

    try:
        # Try using music21 with MuseScore
        score = music21.converter.parse(filename)

        # Configure music21 to use MuseScore
        us = music21.environment.UserSettings()
        us['musicxmlPath'] = '/usr/bin/musescore3'
        us['musescoreDirectPNGPath'] = '/usr/bin/musescore3'

        # Write to PDF
        score.write('musicxml.pdf', fp=pdf_path)
        print(f"✓ Successfully created PDF at {pdf_path}")

    except Exception as e:
        print(f"Error using music21: {str(e)}")
        print("Trying direct conversion with MuseScore...")

        # Fallback: direct conversion with MuseScore
        cmd = f"musescore3 {filename} -o {pdf_path}"
        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)

        if process.returncode != 0:
            print(f"Error converting to PDF: {process.stderr}")
            print("PDF conversion failed.")
            return

        print(f"✓ Successfully created PDF at {pdf_path}")

    # Download the PDF
    files.download(pdf_path)

# Run the conversion function
convert_musicxml_to_pdf()

"""### **Rough Work**"""

# #to Upload a File

# from google.colab import files
# from IPython.display import Audio

# # Step 1: Upload an audio file
# uploaded = files.upload()
# audio_path = list(uploaded.keys())[0]  # Get the uploaded file name

# # Step 2: Play the audio file
# Audio(audio_path)

#Total files in google collab

from google.colab import files
import os
# Step 2: List uploaded files
print("Files in the current directory:")
uploaded_files = os.listdir()
for file in uploaded_files:
    print(file)

# #code to Download file From Collab
# from google.colab import files

# files.download('generated_notation.ly')

# import os
# import shutil

# # Path to the runtime directory
# runtime_dir = "/content/"

# # List all files and directories
# for item in os.listdir(runtime_dir):
#     item_path = os.path.join(runtime_dir, item)

#     # Ensure Google Drive files are not touched
#     if item in ["MyDrive", "drive"]:  # Avoid any Google Drive-related folders
#         continue

#     # Remove only runtime files and folders
#     try:
#         if os.path.isdir(item_path):
#             shutil.rmtree(item_path)
#         else:
#             os.remove(item_path)
#         print(f"Deleted: {item_path}")
#     except Exception as e:
#         print(f"Failed to delete {item_path}: {str(e)}")

# print("All runtime files and folders (except Google Drive) have been deleted safely.")



"""## **Website Development**"""

#Commands to Install FastAPI for the backend and Uvicorn to run the server
!pip install flask flask_cors pyngrok

!pip install pyngrok

# Replace YOUR_AUTHTOKEN with your actual ngrok token
!ngrok authtoken 2u29Eik9sQDmNnswd02jNBLyH7z_JjLvtsiDuttv2WQMvGNM

# Import necessary libraries
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
from pyngrok import ngrok
import os

app = Flask(__name__)
CORS(app)

UPLOAD_FOLDER = "/content/uploads"
OUTPUT_FOLDER = "/content/outputs"

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

@app.route("/upload", methods=["POST"])
def upload_file():
    if "file" not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    filename = secure_filename(file.filename)
    file_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(file_path)

    # Call your existing processing functions here
    processed_pdf = process_audio(file_path)

    if processed_pdf:
        return send_file(processed_pdf, as_attachment=True)
    else:
        return jsonify({"error": "Processing failed"}), 500

def process_audio(file_path):
    """Modify this function to run your existing Colab processing steps."""
    # Here, integrate your existing processing functions for:
    # 1. Preprocessing audio
    # 2. Separating 'other' track with Demucs
    # 3. Detecting guitar sound
    # 4. Converting to MIDI
    # 5. Generating PDF

    output_pdf = "/content/generated_notation.pdf"
    return output_pdf  # Return the path of the generated PDF

# Expose the API with ngrok
public_url = ngrok.connect(5000).public_url
print(f"Your API is running at: {public_url}")

app.run(port=5000)